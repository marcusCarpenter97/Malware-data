import os
import models
import backend
import tensorflow as tf

def model_loader(models_dir="checkpoints"):
    """
    Loads Tensorflow models from a directory, and returns a dictionary where
    the keys are the model names.
    """

    model_names = os.listdir(models_dir)

    models = {}

    for model_name in model_names:
        model_path = os.path.join(models_dir, model_name)
        m = tf.keras.models.load_model(model_path)
        models[model_name] = m

    return models

def prepare_data():
    data = backend.load_data(backend.DATA_DIR)
    lookup_table = backend.load_lookup_table()

    # The data needs to be trimmed because the sequences are too long.
    trimmed_data = []
    data_modes = ['first', 'last', 'random']

    for mode in data_modes:
        trimmed_apis = backend.trim_data(data, mode=mode)
        trimmed_strs = [" ".join(api_seq) for api_seq in trimmed_apis]
        trimmed_data.append(trimmed_strs)

    labels = []
    for batch in data:
        for label in batch['labels']:
            labels.append(label)

    return trimmed_data, labels

def main():
    models = model_loader()  # Load all trained tensorflow models
    models['BiGRU-first']  # Gets one model from list, replace first with last or random for other feature selection methods another good model is the MLP.

    trimmed_data, labels = prepare_data()  # trimmed_data is a list with three sublists each containing all inputs for feature selction modes
    
    for idx, trim in enumerate(trimmed_data):
        train_x, test_x, train_y, test_y = backend.split_train_test(trim, labels)

        vectorize_corpus = tf.data.Dataset.from_tensor_slices(train_x)
        vectorize_layer = models.train_vectorization_layer(vectorize_corpus)

        vocab = vectorize_layer.get_vocabulary()  # The vocabulary for each feature selection is different, and has different sizes.

if __name__ == "__main__":
    main()
