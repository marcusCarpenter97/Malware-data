""" Helper functions for processed data and analysis functions. """
import json
import backend
import numpy as np
#import os
import pandas as pd
#from tabulate import tabulate
import matplotlib.pyplot as plt
#from pprint import pprint
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

DATA_DIR = "/home/cape/Malware-Data/Processed/"

def calculate_api_length_stats(data, min_len=512):
    lengths = []
    for batch in data:
        for sample in batch['apis']:
            lengths.append(len(sample))

    small_count = [l for l in lengths if l < min_len]

    print("API length distribution")
    print("Total samples:", len(lengths))
    print("Mean len:", np.mean(lengths))
    print("Shortest seq:", min(lengths))
    print("Longest seq:", max(lengths))
    print(f"Count of API seq shorter than {min_len}:", len(small_count))

def calculate_mean_tf_idf(corpus, name, vocabulary=None):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus, vocabulary)
    dense_tfidf = tfidf_matrix.todense().tolist()
    features = vectorizer.get_feature_names_out()
    res = pd.DataFrame(dense_tfidf, columns=features)
    res = res.mask(res==0).mean(axis=0)
    res.name = name
    return res.to_frame().reset_index()

def main():

    data = backend.load_data(DATA_DIR)

    lookup_table = load_lookup_table()

    corpus_by_label = dict()
    label_count = dict()

    for label in lookup_table:
        corpus_by_label[label] = []
        label_count[label] = 0

    for txt_label in lookup_table:
        for batch in data:
            for apis, bin_label in zip(batch['apis'], batch['labels']):

                idxs = np.where(bin_label)[0]
                txt_labels = [lookup_table[idx] for idx in idxs]

                if txt_label in txt_labels:
                    corpus_by_label[txt_label].append(apis)
                    label_count[txt_label] += 1
    # TODO calculate tf-idf and frequency here
    total_corpus = [" ".join(sample) for label in corpus_by_label for sample in corpus_by_label[label]]

    temp = []
    tfidf_res = calculate_mean_tf_idf(total_corpus, "Total")
    for label in corpus_by_label:
        label_corpus = [" ".join(sample) for sample in corpus_by_label[label]]
        tf_idf = calculate_mean_tf_idf(label_corpus, label)#, vocabulary=tfidf_res["index"])
        temp.append(tf_idf)
        #tfidf_res = tfidf_res.merge(tf_idf, how="inner")
    print(tfidf_res)
    for i in temp:
        print(i)

if __name__ ==  "__main__":
    main()
    #api_frequency_path = "data-analysis-output/api_frequency_for_category.json"
    #data = load_data(DATA_DIR)
    #tf_idf = calculate_tfidf(data)
    #print(tf_idf)
    #res = tf_idf.mask(tf_idf==0).mean(axis=0)
    #frequency = calculate_api_frequency(data)
    #f_sum = frequency.sum(axis=0)
    #res = pd.DataFrame(res)
    #res[1] = tf_idf.mean(axis=0) 
    #res[2] = f_sum
    #res.columns = (["TF-IDF (zero mask)", "TF-IDF", "Frequency"])
    #print(res.sort_values(by="TF-IDF (zero mask)"))

    #lookup_table = load_lookup_table()

    #labels = []
    #for batch in data:
    #    labels.extend(batch['labels'])

    #res = top_apis_tfidf_labels(tf_idf, labels, lookup_table)
    #plot_top_tfidf(res)

    #with open(api_frequency_path, 'r') as fp:
    #    res = json.load(fp)
    #plot_most_frequent_apis(res)

    #count_labels()
