""" Helper functions for processed data and analysis functions. """
import os
import json
import numpy as np
import pandas as pd
from tabulate import tabulate
import matplotlib.pyplot as plt
from pprint import pprint
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

DATA_DIR = "/home/cape/Malware-Data/Processed/"

def load_lookup_table():
    with open("lookup_table.json", 'r') as lt:
        lookup_table = json.load(lt)
    return lookup_table

def load_data(data_dir):
    """ Returns a list of batch dictionaries. """

    data = []
    data_files = os.listdir(data_dir)

    for data_file in data_files:
        file_path = os.path.join(data_dir, data_file)
        with open(file_path, "r") as fp:
            data.append(json.load(fp))
    return data

def remove_missing_data(data):
    """ Returns a list of batch dictionaries. """

    for batch in data:
        clean_apis = []
        clean_labels = []
        for idx, sample in enumerate(batch['apis']):
            if sample:
                clean_apis.append(sample)
                clean_labels.append(batch['labels'][idx])
        batch['apis'] = clean_apis
        batch['labels'] = clean_labels
    return data

def api_frequency_for_category(data, res_path):

    def increment_api_count(hist, apis, counts):
        for idx, api in enumerate(apis):
            if api in hist:
                hist[api] += counts[idx]
            else:
                hist[api] = counts[idx]

    label_api_count = {}
    lookup_table = load_lookup_table()

    for name in lookup_table:
        label_api_count[name] = {}
    label_api_count['benign'] = {}

    for batch in data:
        for sample_idx, sample in enumerate(batch['apis']):
            label = batch['labels'][sample_idx]
            label_idxs = np.where(label)[0]

            apis, counts = np.unique(sample, return_counts=True)
            counts = counts.tolist()  # JSON cannot encode numpys int64.

            if label_idxs.size == 0:
                increment_api_count(label_api_count['benign'], apis, counts)

            for idx in label_idxs:
                mal_type = lookup_table[idx]
                increment_api_count(label_api_count[mal_type], apis, counts)

    with open(res_path, 'w') as fp:
        json.dump(label_api_count, fp, indent=4)

    return label_api_count

def plot_most_frequent_apis(labeled_api_count):
    """ Sort data to get the top 10 most frequent apis. """
    dfs = {}
    for label in labeled_api_count:
        dfs[label] = pd.DataFrame(labeled_api_count[label].values(), labeled_api_count[label].keys(), columns = ["API count"])

    for label in dfs:
        top_ten = dfs[label].nlargest(10, ["API count"])
        ax = top_ten.plot(kind='barh', title=f"Top 10 {label} APIs")
        ax.legend(['Total count'], loc='best')
        ax.tick_params(axis='x', rotation=20)
        plt.tight_layout()
        fig = ax.get_figure()
        fig.savefig(f"data-analysis-output/API-count-for-{label}.png")

def find_unique_apis_category(labeled_api_count):
    labels = labeled_api_count.keys()
    res = {}

    for cur_label in labels:
        sub_lables = [label for label in labels if label != cur_label]
        temp_res = set(labeled_api_count[cur_label].keys())

        for sub_label in sub_lables:
            temp_res = temp_res - set(labeled_api_count[sub_label].keys())
        res[cur_label] = temp_res

    return res

def calculate_tfidf(data):
    apis = []
    for batch in data:
        apis.extend(batch['apis'])

    corpus = [" ".join(sample) for sample in apis]

    vectorizer = TfidfVectorizer()
    # stop words: common words in english that dilute the calculation of
    # other words. As this is not english defoult values should not be
    # used.  TODO create a list of custom stop words.
    tfidf_matrix = vectorizer.fit_transform(corpus)
    dense_tfidf = tfidf_matrix.todense().tolist()

    features = vectorizer.get_feature_names_out()

    return pd.DataFrame(dense_tfidf, columns=features)

def calculate_api_frequency(data):
    apis = []
    for batch in data:
        apis.extend(batch['apis'])

    corpus = [" ".join(sample) for sample in apis]

    vectorizer = CountVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    dense_tfidf = tfidf_matrix.todense().tolist()

    features = vectorizer.get_feature_names_out()
    return pd.DataFrame(dense_tfidf, columns=features)

def top_apis_tfidf(tfidf, top_n=10):
    mean = tfidf.mean(axis=0)
    mean.index = tfidf.columns
    return mean.nlargest(top_n)

def top_apis_tfidf_labels(tfidf, labels, lookup_table, top_n=10):
    malware_label_idx = {}
    for label in lookup_table:
        malware_label_idx[label] = []
    malware_label_idx['benign'] = []

    for sample_idx, label in enumerate(labels):
        label_idxs = np.where(label)[0]

        if label_idxs.size == 0:
            malware_label_idx['benign'].append(sample_idx)

        for idx in label_idxs:
            mal_type = lookup_table[idx]
            malware_label_idx[mal_type].append(sample_idx)

    tfidf_slices = {}
    for label in malware_label_idx:
        sub_res = tfidf.iloc[malware_label_idx[label]]
        tfidf_slices[label] = sub_res

    result_dict = {}
    for label in tfidf_slices:
        rank = top_apis_tfidf(tfidf_slices[label])
        result_dict[label] = rank
    return result_dict

def plot_top_tfidf(tfidf):
    for label in tfidf:
        ax = tfidf[label].plot(kind='barh', title=f"Top 10 {label} APIs")
        ax.legend(['Tf-Idf'], loc='best')
        plt.tight_layout()
        fig = ax.get_figure()
        fig.savefig(f"data-analysis-output/ifidf-{label}.png")

def count_labels():
    """ Generate table containing count for each label in the data. """
    data = load_data(DATA_DIR)

    labels = [label for batch in data for label in batch["labels"]]
    print(f"Total number of labels found: {len(labels)}")

    data = remove_missing_data(data)

    labels = [label for batch in data for label in batch["labels"]]
    print(f"Number of labels without empty APIs: {len(labels)}")

    lookup_table = load_lookup_table()
    print(f"Labels in look up table: {lookup_table}")

    result = {}
    for name in lookup_table:
        result[name] = 0

    for label in labels:
        idxs = np.where(label)[0]

        for idx in idxs:
            result[lookup_table[idx]] +=1

    temp = {}
    temp["Name"] = result.keys()
    temp["Count"] = result.values()
    print(tabulate(temp, headers="keys", tablefmt="latex_raw"))

if __name__ ==  "__main__":
    api_frequency_path = "data-analysis-output/api_frequency_for_category.json"
    data = load_data(DATA_DIR)
    data = remove_missing_data(data)
    tf_idf = calculate_tfidf(data)
    print(tf_idf)
    res = tf_idf.mask(tf_idf==0).mean(axis=0)
    frequency = calculate_api_frequency(data)
    f_sum = frequency.sum(axis=0)
    res = pd.DataFrame(res)
    res[1] = tf_idf.mean(axis=0) 
    res[2] = f_sum
    res.columns = (["TF-IDF (zero mask)", "TF-IDF", "Frequency"])
    print(res.sort_values(by="TF-IDF (zero mask)"))

    #lookup_table = load_lookup_table()

    #labels = []
    #for batch in data:
    #    labels.extend(batch['labels'])

    #res = top_apis_tfidf_labels(tf_idf, labels, lookup_table)
    #plot_top_tfidf(res)

    #with open(api_frequency_path, 'r') as fp:
    #    res = json.load(fp)
    #plot_most_frequent_apis(res)

    #count_labels()
