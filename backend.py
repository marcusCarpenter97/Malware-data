""" Create malware dataset from CAPE report files. """
import os
import shutil
import json
from datetime import datetime as dt
from sklearn.model_selection import train_test_split
import random 

DATA_DIR = "/home/cape/Malware-Data/Processed/"

def split_train_test(apis, labels):
    return train_test_split(apis, labels, test_size=0.33, random_sate=888)

def trim_data(data, max_length=512, mode='first'):
    """
    data : a list of dictionaries containing batch data.
    max_length : the size of the biggest possible API sequence after trimming.
    mode : the trimming style. Three posibilities, first, last, random.
    mode='first' keeps the fist APIs in the sequence up to max_lenght.
    mode='last' keeps the last APIs in the sequence up to max_lenght.
    mode='random' extracts a max_lenght sized sequences of sequential APIs at a random position.

    Returns a list of lists containing the trimmed APIs.
    """
    trimmed_data = []
    random.seed(888)
    for batch in data:
        if mode == "first":
            trimmed_data.extend([api_seq[:max_length] for api_seq in batch['apis']])
        if mode == "last":
            trimmed_data.extend([api_seq[-max_length:] for api_seq in batch['apis']])
        if mode == "random":
            for api_seq in batch['apis']:
                if len(api_seq) < max_length:
                    trimmed_data.append(api_seq)
                else:
                    offset = random.randint(0, len(api_seq)-max_length)
                    trimmed_data.append(api_seq[offset:offset+max_length])
    return trimmed_data

def load_data(data_dir):
    """ Returns a list of batch dictionaries. """

    data = []
    data_files = os.listdir(data_dir)

    for data_file in data_files:
        file_path = os.path.join(data_dir, data_file)
        with open(file_path, "r") as fp:
            data.append(json.load(fp))
    return data

def load_VT_report(path):
    report = open_json(path)
    if report:
        labels = report.get("data", {}).get("attributes",
                {}).get("popular_threat_classification",
                        {}).get("popular_threat_category", {})
        return [label["value"] for label in labels]
    else:
        return None

def open_json(path):
    try:
        with open(path,  "r") as fp:
            return json.load(fp)

    except json.decoder.JSONDecodeError as err:
        print(f"Could not open {path} - {err}")
        return None

    except FileNotFoundError as err:
        print(f"Could not open - {err}")
        return None

def make_lookup_table(batch_labels):
    """ Create a look up table containing all unique malware classes found in the VirusTotal reports. """
    lookup_table = []

    print("Preparing lookup table...")

    for batch in batch_labels:
        for sample in batch_labels[batch]:
            for malware_label in sample:
                if malware_label not in lookup_table:
                    lookup_table.append(malware_label)

    with open("lookup_table.json", "w") as fp:
        json.dump(lookup_table, fp)

    print("Lookup table saved.")
    return lookup_table

def make_labels(batch_labels, lookup_table):
    """ Make label for a malware sample based on the contents of the corresponding VirusTotal report. """

    processed_labels = dict()

    for batch in batch_labels:
        print(f"Making labels for batch: {batch}")

        processed_labels[batch] = []

        for sample in batch_labels[batch]:
            label = [0 for _ in range(len(lookup_table))]

            for malware_label in sample:
                idx = lookup_table.index(malware_label)
                label[idx] = 1

            processed_labels[batch].append(label)

    return processed_labels

def extract_apis(report):
    timestamps = []
    apis = []
    for process in report.get("behavior", {}).get("processes", {}):
        for call in process.get("calls", {}):
            timestamps.append(call["timestamp"])
            apis.append(call["api"])

    datetime_format = "%Y-%m-%d %H:%M:%S,%f"
    times = [dt.strptime(timestamp, datetime_format) for timestamp in timestamps]

    return [api for _, api in sorted(zip(times, apis))]

def save_data(batch_name, year, apis, labels, save_dir):
    data = {}
    data["name"] = batch_name
    data["year"] = year
    data["apis"] = apis
    data["labels"] = labels

    path = os.path.join(save_dir, f"{batch_name}.json")
    print(f"Saving data to {path}")
    with open(path, "w") as fp:
        json.dump(data, fp, indent=4)

def from_cape_to_full(batch, start, stop):
    """ Move files from CAPE directories into the Full data directory.
        One batch at a time. """
    base_cape_report_path = "/opt/CAPEv2/storage/analyses/"
    end_cape_report_path = "reports/report.json"
    base_cape_dir = "CAPE_"
    base_target = "../Malware-Data/Full"
    target_name = f"{base_cape_dir}{batch}"

    target_dir = os.path.join(base_target,target_name)

    for report_idx in range(start, stop):
        report_path = os.path.join(base_cape_report_path, str(report_idx), end_cape_report_path)

        report_data = open_json(report_path)

        if not report_data:
            continue

        md5 = report_data.get("target", {}).get("file", {}).get("md5", {})
        target_name = os.path.join(target_dir, md5)

        print(f"Copying {report_path} to {target_name}")

        try:
            shutil.copy(report_path, f"{target_name}.json")
        except FileNotFoundError as err:
            print(f"Could not copy - {err}")

def load_full_data():
    """ Load Full data into memory, but only relevant parts such as APIs and labels. """

    base_target = "/home/cape/Malware-Data/Full"
    base_cape_dir = "CAPE_"
    base_vt_dir = "VT_"
    benign = "benign"
    malware = "malware"
    full_dirs = os.listdir(base_target)
    cape_dirs = [sub_dir for sub_dir in full_dirs if base_cape_dir in sub_dir]

    batch_apis = dict()
    batch_labels = dict()

    for cape_dir in cape_dirs:
        batch_path = os.path.join(base_target, cape_dir)
        batch_files = os.listdir(batch_path)

        batch_name = cape_dir.removeprefix(base_cape_dir)

        batch_apis[batch_name] = []
        batch_labels[batch_name] = []

        print(f"\n{batch_name}\n")
        batch_size = len(batch_files)

        for idx, cape_report in enumerate(batch_files):
            print(f"{idx}/{batch_size}", end="\r")

            report_path = os.path.join(batch_path, cape_report)
            report_data = open_json(report_path)

            if not report_data:
                continue

            sample_apis = extract_apis(report_data)

            if not sample_apis:
                continue

            batch_apis[batch_name].append(sample_apis)
            
            vt_report_path = os.path.join(base_target, f"{base_vt_dir}{batch_name}", cape_report)
            labels = load_VT_report(vt_report_path)

            if not labels:
                if benign in cape_dir:
                    labels = [benign]
                else:
                    labels = [malware]

            batch_labels[batch_name].append(labels)

    return batch_apis, batch_labels

def cape_to_full():
    with open("cape-ranges.json", "r") as fp:
        cape_ranges = json.load(fp)

    target_batch = "368" 

    from_cape_to_full(target_batch, cape_ranges[target_batch]["start"], cape_ranges[target_batch]["stop"])

if __name__ == "__main__":

    # TODO: Filter Full dataset into the Clean directory.
    # Clean data separates the batches in directories and each sample in a JSON
    # file. Each sample contains the timestamp and API sequences for all
    # processes, and text labels from VirusTotal. """

    #vt_389 = "/home/cape/Malware-Data/Full/VirusShare_00389"
    #vt_368 = "/home/cape/Malware-Data/Full/VirusShare_00368"
    #cape_389 = "/home/cape/Malware-Data/Clean/CAPE_389"
    #cape_benign = "/home/cape/Malware-Data/Clean/CAPE_benign"

    apis, txt_labels = load_full_data()
    lookup_table = make_lookup_table(txt_labels)
    labels = make_labels(txt_labels, lookup_table)

    save_dir = "/home/cape/Malware-Data/Processed"
    with open("cape-ranges.json", "r") as fp:
        cape_ranges = json.load(fp)

    for batch_name in apis:
        save_data(batch_name, cape_ranges[batch_name]["year"], apis[batch_name], labels[batch_name], save_dir)
