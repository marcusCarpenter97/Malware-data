""" Test different word embeddings on the malware data. """
import random
import numpy as np
import analysis as ana
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense, LSTM, GRU, Bidirectional
from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix

def prepare_dataset_for_models(data, y_percent=0.1, shuffle=False):
    corpus_x = []
    corpus_y = []
    labels_x = []
    labels_y = []

    for batch in data:
        total_sample_num = len(batch['apis'])
        num_y_samples = int(total_sample_num * y_percent)

        y_idxs = random.sample(list(range(total_sample_num)), num_y_samples)

        for idx, sample in enumerate(batch['apis']):
            if idx in y_idxs:
                corpus_y.append(sample)
                labels_y.append(batch['labels'][idx])
            else:
                corpus_x.append(sample)
                labels_x.append(batch['labels'][idx])

    if shuffle:
        train = list(zip(corpus_x, labels_x))
        test = list(zip(corpus_y, labels_y))
        random.shuffle(train)
        random.shuffle(test)
        corpus_x[:], labels_x[:] = zip(*train)
        corpus_y[:], labels_y[:] = zip(*test)

    corpus_x = [" ".join(sample) for sample in corpus_x]
    corpus_y = [" ".join(sample) for sample in corpus_y]

    return corpus_x, corpus_y, labels_x, labels_y

def simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def evaluate_model(labels, predictions):
    res = accuracy_score(labels, predictions.round())
    print("Accuracy score:", res)

    res = f1_score(labels, predictions.round(), average=None)
    print("F1 score:", res)

    res = f1_score(labels, predictions.round(), average='samples')
    print("F1 score (samples):", res)

    res = multilabel_confusion_matrix(labels, predictions.round())
    print("Confusion matrix:\n", res)

def complie_train_predict(model, train_data, test_data, callbacks):
    model.compile(optimizer="adam", loss=tf.losses.BinaryCrossentropy(), metrics=["accuracy"])

    model.fit(train_data, validation_data=test_data, epochs=15, callbacks=callbacks)

    return model.predict(test_data.map(lambda x, y: x))

def create_checkpoint_callback(checkpoint_path):
    return tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
            save_weights_only=True, monitor='val_accuracy', mode='max',
            save_best_only=True)

if __name__ == "__main__":

    np.random.seed(775)

    # Load original data from files.
    data = ana.load_data(ana.DATA_DIR)
    data = ana.remove_missing_data(data)

    corpus_x, corpus_y, labels_x, labels_y = prepare_dataset_for_models(data, shuffle=True)

    for i in range(3):
        corpus_x.pop()
        labels_x.pop()

    n_batches = 5
    batch_size = len(corpus_x) / n_batches

    # Make Tensorflow datasets
    train_text = tf.data.Dataset.from_tensor_slices(corpus_x)
    train_ds = tf.data.Dataset.from_tensor_slices((corpus_x, labels_x)).batch(n_batches)
    test_ds = tf.data.Dataset.from_tensor_slices((corpus_y, labels_y))

    vectorize_layer = TextVectorization(standardize="lower", output_mode='int')
    vectorize_layer.adapt(train_text)
    vocab_size = vectorize_layer.vocabulary_size()

    n_timesteps = 1
    n_features = embedding_dimension = 16
    number_of_outputs = len(labels_x[0])

    # Create and train models
    sem_checkpoint = create_checkpoint_callback("checkpoints/sem")
    sem = simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension)
    sem_preds = complie_train_predict(sem, train_ds, test_ds, [sem_checkpoint])

    mlp_checkpoint = create_checkpoint_callback("checkpoints/mlp")
    mlp = multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension)
    mlp_preds = complie_train_predict(mlp, train_ds, test_ds, [mlp_checkpoint])

    lstm_checkpoint = create_checkpoint_callback("checkpoints/lstm")
    lstm = lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    lstm_preds = complie_train_predict(lstm, train_ds, test_ds, [lstm_checkpoint])

    gru_checkpoint = create_checkpoint_callback("checkpoints/gru")
    gru = gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    gru_preds = complie_train_predict(gru, train_ds, test_ds, [gru_checkpoint])

    bi_lstm_checkpoint = create_checkpoint_callback("checkpoints/bi_lstm")
    bi_lstm = bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    bi_lstm_preds = complie_train_predict(bi_lstm, train_ds, test_ds, [bi_lstm_checkpoint])

    bi_gru_checkpoint = create_checkpoint_callback("checkpoints/bi_gru")
    bi_gru = bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    bi_gru_preds = complie_train_predict(bi_gru, train_ds, test_ds, [bi_gru_checkpoint])

    print("Simple Embedding model:")
    evaluate_model(labels_y, sem_preds)
    print("Multilayer preceptron model:")
    evaluate_model(labels_y, mlp_preds)
    print("LSTM model:")
    evaluate_model(labels_y, lstm_preds)
    print("GRU model:")
    evaluate_model(labels_y, gru_preds)
    print("Bi LSTM model:")
    evaluate_model(labels_y, bi_lstm_preds)
    print("Bi GRU model:")
    evaluate_model(labels_y, bi_gru_preds)
