import models
import backend
#import analysis
#import numpy as np
#import pandas as pd
#import matplotlib.pyplot as plt

# Make data pipeline

# Create models from configuration

# Make model pipeline

# For each data slice:

#  For each model:

#   Train model on data

#   Save results

# Create tables and plots from saved results

def main():

    data = backend.load_data(backend.DATA_DIR)

    # TODO Load model configuration, what is there to load?

    # The data needs to be trimmed because the sequences are too long.
    trimmed_data = []
    data_modes = ['first', 'last', 'random']

    for mode in data_modes:
        trimmed_data.append(backend.trim_data(data, mode=mode))

    models = [models.simple_embedding_model(), models.multilayer_perceptron_model()]

    labels = []
    for batch in data:
        for label in batch['labels']:
            labels.append(label)

    for trim in trimmed_data:
        train_x, test_x, train_y, test_y = split_train_test(trim, labels)

        # TODO data needs to be transformed into Tensorflow dataset.
        vectorize_corpus = tf.data.Dataset.from_tensor_slices(train_x)
        vectorize_layer = models.train_vectorization_layer(vectorize_corpus)

        for model in models:
            break  # TODO compile model, train model, evaluate model.

if __name__ == "__main__":
    main()
