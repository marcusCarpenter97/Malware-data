""" Test different word embeddings on the malware data. """
import random
import numpy as np
import analysis as ana
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense, LSTM, GRU, Bidirectional
from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix

def prepare_dataset_for_models(data, y_percent=0.1, shuffle=False):
    corpus_x = []
    corpus_y = []
    labels_x = []
    labels_y = []

    for batch in data:
        total_sample_num = len(batch['apis'])
        num_y_samples = int(total_sample_num * y_percent)

        y_idxs = random.sample(list(range(total_sample_num)), num_y_samples)

        for idx, sample in enumerate(batch['apis']):
            if idx in y_idxs:
                corpus_y.append(sample)
                labels_y.append(batch['labels'][idx])
            else:
                corpus_x.append(sample)
                labels_x.append(batch['labels'][idx])

    if shuffle:
        train = list(zip(corpus_x, labels_x))
        test = list(zip(corpus_y, labels_y))
        random.shuffle(train)
        random.shuffle(test)
        corpus_x[:], labels_x[:] = zip(*train)
        corpus_y[:], labels_y[:] = zip(*test)

    corpus_x = [" ".join(sample) for sample in corpus_x]
    corpus_y = [" ".join(sample) for sample in corpus_y]

    return corpus_x, corpus_y, labels_x, labels_y

def simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def evaluate_model(labels, predictions):
    res = accuracy_score(labels, predictions.round())
    print("Accuracy score:", res)

    res = f1_score(labels, predictions.round(), average=None)
    print("F1 score:", res)

    res = f1_score(labels, predictions.round(), average='samples')
    print("F1 score (samples):", res)

    res = multilabel_confusion_matrix(labels, predictions.round())
    print("Confusion matrix:\n", res)

def complie_train_predict(model, train_data, test_data, callbacks):
    model.compile(optimizer="adam", loss=tf.losses.BinaryCrossentropy(), metrics=["accuracy"])

    model.fit(train_data, validation_data=test_data, epochs=15, callbacks=callbacks)

    return model.predict(test_data.map(lambda x, y: x))

def create_checkpoint_callback(checkpoint_path):
    return tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
            save_weights_only=True, monitor='val_accuracy', mode='max',
            save_best_only=True)

def save_model(model, preds, path):
    model.save(path)
    with open(f"{path}.npy", "wb") as f:
        np.save(f, preds)

if __name__ == "__main__":

    np.random.seed(775)

    # Load original data from files.
    data = ana.load_data(ana.DATA_DIR)
    data = ana.remove_missing_data(data)

    corpus_x, corpus_y, labels_x, labels_y = prepare_dataset_for_models(data, shuffle=True)

    # Remove last three samples to make divisible by a reasonable batch number.
    for i in range(2):
        corpus_x.pop()
        labels_x.pop()

    print()
    print("Corpus x len:", len(corpus_x))
    print("Corpus y len:", len(corpus_y))
    print("Labels x len:", len(labels_x))
    print("Labels y len:", len(labels_y))
    print()

    n_batches = 4
    batch_size = len(corpus_x) / n_batches

    # Make Tensorflow datasets
    train_text = tf.data.Dataset.from_tensor_slices(corpus_x)
    train_ds = tf.data.Dataset.from_tensor_slices((corpus_x, labels_x)).batch(n_batches)
    test_ds = tf.data.Dataset.from_tensor_slices((corpus_y, labels_y)).batch(2)

    print()
    print("Train text info:", train_text.element_spec)
    print("Train ds info:", train_ds.element_spec)
    print("Test ds info:", test_ds.element_spec)
    print()

    vectorize_layer = TextVectorization(standardize="lower", output_mode='int')
    vectorize_layer.adapt(train_text)
    vocab_size = vectorize_layer.vocabulary_size()

    n_timesteps = 1
    n_features = embedding_dimension = 16
    number_of_outputs = len(labels_x[0])

    print()
    print("Number of timesteps:", n_timesteps)
    print("Number of features:", n_features)
    print("Embedding dimension:", embedding_dimension)
    print("Number of outputs:", number_of_outputs)
    print("Vocabulary size", vocab_size)
    print()

    # Create and train models
    sem_checkpoint = create_checkpoint_callback("checkpoints/sem")
    sem = simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension)
    sem_preds = complie_train_predict(sem, train_ds, test_ds, [sem_checkpoint])
    save_model(sem, sem_preds, "saved_models/sem")

    mlp_checkpoint = create_checkpoint_callback("checkpoints/mlp")
    mlp = multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension)
    mlp_preds = complie_train_predict(mlp, train_ds, test_ds, [mlp_checkpoint])
    save_model(mlp, mlp_preds, "saved_models/mlp")

    lstm_checkpoint = create_checkpoint_callback("checkpoints/lstm")
    lstm = lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    lstm_preds = complie_train_predict(lstm, train_ds, test_ds, [lstm_checkpoint])
    save_model(lstm, lstm_preds, "saved_models/lstm")

    gru_checkpoint = create_checkpoint_callback("checkpoints/gru")
    gru = gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    gru_preds = complie_train_predict(gru, train_ds, test_ds, [gru_checkpoint])
    save_model(gru, gru_preds, "saved_models/gru")

    bi_lstm_checkpoint = create_checkpoint_callback("checkpoints/bi_lstm")
    bi_lstm = bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    bi_lstm_preds = complie_train_predict(bi_lstm, train_ds, test_ds, [bi_lstm_checkpoint])
    save_model(bi_lstm, bi_lstm_preds, "saved_models/bilstm")

    bi_gru_checkpoint = create_checkpoint_callback("checkpoints/bi_gru")
    bi_gru = bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    bi_gru_preds = complie_train_predict(bi_gru, train_ds, test_ds, [bi_gru_checkpoint])
    save_model(bi_gru, bi_gru_preds, "saved_models/bigru")

