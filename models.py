""" Test different word embeddings on the malware data. """
import json
import random
import numpy as np
import backend
import tensorflow as tf
import matplotlib.pyplot as plt
from tabulate import tabulate
from tensorflow.keras import Sequential
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense, LSTM, GRU, Bidirectional
from sklearn.metrics import accuracy_score, f1_score, multilabel_confusion_matrix, ConfusionMatrixDisplay

def prepare_dataset_for_models(data, y_percent=0.1, shuffle=False):  # TODO remove this.
    corpus_x = []
    corpus_y = []
    labels_x = []
    labels_y = []

    for batch in data:
        total_sample_num = len(batch['apis'])
        num_y_samples = int(total_sample_num * y_percent)

        y_idxs = random.sample(list(range(total_sample_num)), num_y_samples)

        for idx, sample in enumerate(batch['apis']):
            if idx in y_idxs:
                corpus_y.append(sample)
                labels_y.append(batch['labels'][idx])
            else:
                corpus_x.append(sample)
                labels_x.append(batch['labels'][idx])

    if shuffle:
        train = list(zip(corpus_x, labels_x))
        test = list(zip(corpus_y, labels_y))
        random.shuffle(train)
        random.shuffle(test)
        corpus_x[:], labels_x[:] = zip(*train)
        corpus_y[:], labels_y[:] = zip(*test)

    corpus_x = [" ".join(sample) for sample in corpus_x]
    corpus_y = [" ".join(sample) for sample in corpus_y]

    return corpus_x, corpus_y, labels_x, labels_y

def train_vectorization_layer(corpus):
    vectorize_layer = TextVectorization(standardize="lower", output_mode='int')
    vectorize_layer.adapt(corpus)
    return vectorize_layer

def simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension):  # TODO remove vocab_size from all models and use, vectorize_layber.vocabulary_size()
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features):
    model = Sequential()

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def acc_v_f1(labels, preds):
    headers = ["Models", "Accuracy", "F1-score"]
    table = []
    for model in preds:
        acc = accuracy_score(labels, preds[model].round())
        fone = f1_score(labels, preds[model].round(), average='samples', zero_division=1)
        table.append([model, acc, fone])
    txt = tabulate(table, headers=headers, tablefmt="latex_raw")

    with open("acc_v_f1.txt", 'w') as fp:
        fp.write(txt)

def f1_categories(labels, preds):

    with open("lookup_table.json") as lt:
        headers = json.load(lt)

    headers.insert(0, "Models")

    table = []
    for model in preds:
        row = list(f1_score(labels, preds[model].round(), average=None, zero_division=1))
        row.insert(0, model)
        table.append(row)
    txt = tabulate(table, headers=headers,  tablefmt="latex_raw")

    with open("f1_categories.txt", 'w') as fp:
        fp.write(txt)

def confusion_matrix(labels, preds, name):

    n_rows = 3
    n_cols = 5

    with open("lookup_table.json") as lt:
        headers = json.load(lt)

    cm = multilabel_confusion_matrix(labels, preds.round())

    f, axes = plt.subplots(n_rows, n_cols, constrained_layout=True)
    axes = axes.ravel()
    for idx, subm in enumerate(cm):
        disp = ConfusionMatrixDisplay(subm)
        disp.plot(ax=axes[idx])
        disp.im_.colorbar.remove()
        disp.ax_.set_title(f'{headers[idx]}')
        if idx<10:  # 10 is the count of plots to skip until x axis labels are displayed. Good for multiple rows.
            disp.ax_.set_xlabel('')
        if idx%n_cols!=0:
            disp.ax_.set_ylabel('')

    f.colorbar(disp.im_, ax=axes)
    plt.savefig(f"{name}_conf_matrix.png", bbox_inches="tight")

def evaluate_models(labels, model_preds):
    acc_v_f1(labels, model_preds)
    f1_categories(labels, model_preds)

    for model in model_preds:
        confusion_matrix(labels, model_preds[model], model)

def complie_train_predict(model, train_data, test_data, callbacks):

    adam = tf.keras.optimizers.Adam(clipnorm=1.0)

    model.compile(optimizer=adam, loss=tf.losses.BinaryCrossentropy(), metrics=["accuracy"])

    model.fit(train_data, validation_data=test_data, epochs=15, callbacks=callbacks)

    return model.predict(test_data.map(lambda x, y: x))

def create_checkpoint_callback(checkpoint_path):
    return tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
            save_weights_only=True, monitor='val_accuracy', mode='max',
            save_best_only=True)

def create_earlystopping_callback():
    return tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)

def save_model(model, preds, path):
    # TODO delete this
    model.save(path)
    with open(f"{path}.npy", "wb") as f:
        np.save(f, preds)

if __name__ == "__main__":

    np.random.seed(775)

    # Load original data from files.
    data = backend.load_data(backend.DATA_DIR)

    corpus_x, corpus_y, labels_x, labels_y = prepare_dataset_for_models(data, shuffle=True)

    print()
    print("Corpus x len:", len(corpus_x))
    print("Corpus y len:", len(corpus_y))
    print("Labels x len:", len(labels_x))
    print("Labels y len:", len(labels_y))
    print()

    batch_size = 2
    print("Batch size:", batch_size)
    print()
    #batch_size = int(len(corpus_x) / n_batches)

    # Make Tensorflow datasets
    train_text = tf.data.Dataset.from_tensor_slices(corpus_x)
    train_ds = tf.data.Dataset.from_tensor_slices((corpus_x, labels_x)).batch(batch_size)
    test_ds = tf.data.Dataset.from_tensor_slices((corpus_y, labels_y)).batch(batch_size)

    train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
    test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

    print()
    print("Train text info:", train_text.element_spec)
    print("Train ds info:", train_ds.element_spec)
    print("Test ds info:", test_ds.element_spec)
    print()

    vectorize_layer = TextVectorization(standardize="lower", output_mode='int')
    vectorize_layer.adapt(train_text)
    vocab_size = vectorize_layer.vocabulary_size()

    n_timesteps = 1
    n_features = embedding_dimension = 16
    number_of_outputs = len(labels_x[0])

    print()
    print("Number of timesteps:", n_timesteps)
    print("Number of features:", n_features)
    print("Embedding dimension:", embedding_dimension)
    print("Number of outputs:", number_of_outputs)
    print("Vocabulary size", vocab_size)
    print()

    results = dict()

    # Create and train models
    sem_checkpoint = create_checkpoint_callback("checkpoints/sem")
    sem_earlystop = create_earlystopping_callback()
    sem = simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension)
    results["sem"] = complie_train_predict(sem, train_ds, test_ds, [sem_checkpoint, sem_earlystop])

    mlp_checkpoint = create_checkpoint_callback("checkpoints/mlp")
    mlp_earlystop = create_earlystopping_callback()
    mlp = multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension)
    results["mlp"] = complie_train_predict(mlp, train_ds, test_ds, [mlp_checkpoint, mlp_earlystop])

    lstm_checkpoint = create_checkpoint_callback("checkpoints/lstm")
    lstm_earlystop = create_earlystopping_callback()
    lstm = lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    results["lstm"] = complie_train_predict(lstm, train_ds, test_ds, [lstm_checkpoint, lstm_earlystop])

    gru_checkpoint = create_checkpoint_callback("checkpoints/gru")
    gru_earlystop = create_earlystopping_callback()
    gru = gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    results["gru"] = complie_train_predict(gru, train_ds, test_ds, [gru_checkpoint, gru_earlystop])

    bi_lstm_checkpoint = create_checkpoint_callback("checkpoints/bi_lstm")
    bi_lstm_earlystop = create_earlystopping_callback()
    bi_lstm = bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    results["bi_lstm"] = complie_train_predict(bi_lstm, train_ds, test_ds, [bi_lstm_checkpoint, bi_lstm_earlystop])

    bi_gru_checkpoint = create_checkpoint_callback("checkpoints/bi_gru")
    bi_gru_earlystop = create_earlystopping_callback()
    bi_gru = bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features)
    results["bi_gru"] = complie_train_predict(bi_gru, train_ds, test_ds, [bi_gru_checkpoint, bi_gru_earlystop])

    evaluate_models(labels_y, results)
