""" Test different word embeddings on the malware data. """
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense, LSTM, GRU, Bidirectional

def train_vectorization_layer(corpus):
    vectorize_layer = TextVectorization(standardize="lower", output_mode='int')
    vectorize_layer.adapt(corpus)
    return vectorize_layer

def simple_embedding_model(vectorize_layer, vocab_size, embedding_dimension, number_of_outputs):
    model = Sequential(name="SEM")

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def multilayer_perceptron_model(vectorize_layer, vocab_size, embedding_dimension, number_of_outputs):
    model = Sequential(name="MLP")

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GlobalAveragePooling1D())

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features, number_of_outputs):
    model = Sequential(name="LSTM")

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features, number_of_outputs):
    model = Sequential(name="GRU")

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features)))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_lstm_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features, number_of_outputs):
    model = Sequential(name="BiLSTM")

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model

def bi_gru_model(vectorize_layer, vocab_size, embedding_dimension, batch_size, n_timesteps, n_features, number_of_outputs):
    model = Sequential(name="BiGRU")

    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    
    model.add(vectorize_layer)

    model.add(Embedding(vocab_size, embedding_dimension))

    model.add(Bidirectional(GRU(64, batch_input_shape=(batch_size, n_timesteps, n_features))))

    model.add(Dense(64, activation="relu"))

    model.add(Dense(number_of_outputs, activation="sigmoid"))

    return model
